{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7145515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Deep Learning.\n",
    "\n",
    "#a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\n",
    "\n",
    "\"\"\"Here's an example of how you can build a Deep Neural Network (DNN) with five hidden layers, each consisting of\n",
    "   100 neurons. We'll use the He initialization method and the ELU activation function.\n",
    "   \n",
    "   import tensorflow as tf\n",
    "\n",
    "# Define the DNN architecture\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(tf.keras.layers.Dense(units=100, activation='elu', kernel_initializer='he_normal', input_shape=(input_shape,)))\n",
    "\n",
    "# Hidden layers\n",
    "for _ in range(4):\n",
    "    model.add(tf.keras.layers.Dense(units=100, activation='elu', kernel_initializer='he_normal'))\n",
    "\n",
    "# Output layer\n",
    "model.add(tf.keras.layers.Dense(units=output_units, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "model.summary()\n",
    "\n",
    "  In this example, you need to replace input_shape with the shape of your input data and output_units with the number\n",
    "  of units/neurons in your output layer.\n",
    "\n",
    "  The Dense layers are used to create fully connected layers. The kernel_initializer='he_normal' argument initializes \n",
    "  the weights using the He initialization method, which is a popular initialization technique for deep neural networks. \n",
    "  The activation='elu' argument sets the activation function of each layer to the Exponential Linear Unit (ELU) activation\n",
    "  function.\n",
    "\n",
    "  The model is compiled with the Adam optimizer, categorical cross-entropy loss function (assuming multi-class \n",
    "  classification), and accuracy as the evaluation metric.\n",
    "\n",
    "  Finally, calling model.summary() will provide a summary of the model's architecture, showing the number of \n",
    "  parameters in each layer.\n",
    "\n",
    "  Feel free to modify the code based on your specific requirements and data.\"\"\"\n",
    "\n",
    "#b. Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use \n",
    "transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, \n",
    "and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.\n",
    "\n",
    "\"\"\"Here's an example of how you can train a Deep Neural Network (DNN) with Adam optimization and early stopping on \n",
    "   the MNIST dataset, specifically on digits 0 to 4. The model will have a softmax output layer with five neurons.\n",
    "   \n",
    "   import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Filter digits 0 to 4\n",
    "train_filter = y_train <= 4\n",
    "test_filter = y_test <= 4\n",
    "X_train, y_train = X_train[train_filter], y_train[train_filter]\n",
    "X_test, y_test = X_test[test_filter], y_test[test_filter]\n",
    "\n",
    "# Normalize input data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=5)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=5)\n",
    "\n",
    "# Define the DNN architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define checkpoints and early stopping\n",
    "checkpoint = ModelCheckpoint('mnist_dnn_checkpoint.h5', save_best_only=True)\n",
    "early_stopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test),\n",
    "                    callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "# Save the final model\n",
    "model.save('mnist_dnn_final_model.h5')\n",
    "\n",
    "\n",
    "   In this example, we first load the MNIST dataset using mnist.load_data() and filter the data to only include \n",
    "   digits 0 to 4. The input data is then normalized by dividing by 255.0.\n",
    "\n",
    "   Next, the labels are converted to one-hot encoding using tf.keras.utils.to_categorical() to match the softmax \n",
    "   output layer's requirements.\n",
    "\n",
    "   The DNN architecture consists of four hidden layers, each with 100 neurons, using the ELU activation function\n",
    "   and He initialization. The output layer has five neurons with a softmax activation function.\n",
    "\n",
    "   The model is compiled with the Adam optimizer, categorical cross-entropy loss function, and accuracy as the\n",
    "   evaluation metric.\n",
    "\n",
    "   The code includes the ModelCheckpoint callback to save the best model during training based on validation accuracy. \n",
    "   The EarlyStopping callback is used to stop training early if the validation loss does not improve for five consecutive\n",
    "   epochs, and it restores the weights of the best model.\n",
    "\n",
    "   During training, the checkpoints are saved with the name \"mnist_dnn_checkpoint.h5\", and the final model is saved as\n",
    "   \"mnist_dnn_final_model.h5\".\n",
    "\n",
    "   Feel free to modify the code based on your specific requirements and preferences.\"\"\"\n",
    "\n",
    "#c. Tune the hyperparameters using cross-validation and see what precision you can achieve.\n",
    "\n",
    "\"\"\"To tune the hyperparameters of the Deep Neural Network (DNN) and evaluate its precision using cross-validation, \n",
    "   you can use Scikit-learn's GridSearchCV or RandomizedSearchCV classes. Here's an example of how you can perform \n",
    "   hyperparameter tuning and evaluate precision using cross-validation:\n",
    "   \n",
    "   import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Define a function to create the DNN model\n",
    "def create_model(learning_rate=0.001, num_neurons=100, activation='elu', kernel_initializer='he_normal'):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(num_neurons, activation=activation, kernel_initializer=kernel_initializer),\n",
    "        tf.keras.layers.Dense(num_neurons, activation=activation, kernel_initializer=kernel_initializer),\n",
    "        tf.keras.layers.Dense(num_neurons, activation=activation, kernel_initializer=kernel_initializer),\n",
    "        tf.keras.layers.Dense(num_neurons, activation=activation, kernel_initializer=kernel_initializer),\n",
    "        tf.keras.layers.Dense(5, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the KerasClassifier for Scikit-learn compatibility\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define the hyperparameter grid for search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'num_neurons': [50, 100, 150],\n",
    "    'activation': ['elu', 'relu'],\n",
    "    'kernel_initializer': ['he_normal', 'glorot_uniform']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='precision_macro')\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best precision and hyperparameters\n",
    "print(\"Best Precision: {:.4f}\".format(grid_result.best_score_))\n",
    "print(\"Best Hyperparameters:\", grid_result.best_params_)\n",
    "   \n",
    "   \n",
    "   In this example, we define a function create_model() that builds the DNN model with the specified hyperparameters. \n",
    "   The hyperparameters to be tuned include the learning rate, number of neurons, activation function, and kernel initializer.\n",
    "\n",
    "   We then create a KerasClassifier object from the function create_model() to make it compatible with Scikit-learn's \n",
    "   cross-validation utilities.\n",
    "\n",
    "   The param_grid dictionary defines the grid of hyperparameters to search over. You can modify this grid to include\n",
    "   more hyperparameters or adjust the ranges.\n",
    "\n",
    "   The GridSearchCV class is used to perform the grid search with cross-validation. We specify the number of cross-validation\n",
    "   folds (cv=3) and the scoring metric (scoring='precision_macro') to evaluate the precision. You can adjust these parameters \n",
    "   as needed.\n",
    "\n",
    "   After performing the grid search, the best precision score and corresponding hyperparameters are printed.\n",
    "\n",
    "   Feel free to modify the code to include additional hyperparameters, adjust the ranges, or change the scoring metric\n",
    "   based on your specific requirements.\"\"\"\n",
    "\n",
    "#d. Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it\n",
    "produce a better model?\n",
    "\n",
    "\"\"\"To compare the learning curves and assess the impact of Batch Normalization on convergence speed and model performance,\n",
    "   we can modify the Deep Neural Network (DNN) by adding Batch Normalization layers. Here's an example:\n",
    "   \n",
    "   import tensorflow as tf\n",
    "\n",
    "# Define the DNN architecture with Batch Normalization\n",
    "model_bn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with Batch Normalization\n",
    "history_bn = model_bn.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Define the DNN architecture without Batch Normalization\n",
    "model_no_bn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_no_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model without Batch Normalization\n",
    "history_no_bn = model_no_bn.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "  \n",
    "  \n",
    "  In this example, we create two models: model_bn with Batch Normalization layers and model_no_bn without Batch \n",
    "  Normalization layers.\n",
    "\n",
    "  For model_bn, Batch Normalization layers are added after each dense layer. Batch Normalization helps normalize the\n",
    "  inputs to each layer, which can accelerate training and improve generalization.\n",
    "\n",
    "  Both models are trained using the same training data, epochs, batch size, and validation data. The training process\n",
    "  and learning curves are captured in the history_bn and history_no_bn variables, respectively.\n",
    "\n",
    "  To compare the learning curves, you can plot the training and validation loss and accuracy for both models:\n",
    "  \n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the learning curves\n",
    "def plot_learning_curves(history, title):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(title + ' - Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(title + ' - Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot learning curves for models with and without Batch Normalization\n",
    "plot_learning_curves(history_bn, 'Model with Batch Normalization')\n",
    "plot_learning_curves(history_no_bn, 'Model without Batch Normalization')\n",
    "\n",
    "\n",
    "   This code will generate two subplots showing the learning curves for both models, one for loss and another for accuracy.\n",
    "\n",
    "  By comparing the learning curves, you can assess if the model with Batch Normalization converges faster and produces\n",
    "  a better model in terms of training and validation performance.\n",
    "\n",
    "  Feel free to modify the code to adjust the model architecture or training parameters according to your need.\"\"\"\n",
    "\n",
    "#e. Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?\n",
    "\n",
    "\"\"\"To address the issue of overfitting in the model, we can add dropout regularization to every layer of the Deep Neural \n",
    "   Network (DNN). Dropout randomly sets a fraction of input units to 0 at each update during training, which helps prevent \n",
    "   overfitting. Here's an example of how you can add dropout to each layer:\n",
    "   \n",
    "   import tensorflow as tf\n",
    "\n",
    "# Define the DNN architecture with Dropout regularization\n",
    "model_dropout = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with Dropout regularization\n",
    "history_dropout = model_dropout.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "  In this example, we create a new model called model_dropout by adding Dropout layers after each dense layer. \n",
    "  The Dropout layers have a dropout rate of 0.2, meaning 20% of the input units will be randomly set to 0 during training.\n",
    "\n",
    "  The rest of the code remains the same as in the previous examples, compiling the model with Adam optimizer and \n",
    "  training the model with dropout regularization.\n",
    "\n",
    "  To compare the learning curves between the model with dropout and the previous models, you can use the plot_learning_\n",
    "  curves() function mentioned in the previous response:\n",
    "  \n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot learning curves for models with and without Dropout\n",
    "plot_learning_curves(history_dropout, 'Model with Dropout')\n",
    "\n",
    "\n",
    "   By plotting the learning curves, you can assess if the addition of dropout regularization helps in reducing overfitting\n",
    "   and improving the model's generalization performance.\n",
    "\n",
    "  Feel free to modify the dropout rate or other model parameters to experiment and find the optimal settings for your \n",
    "  specific task.\"\"\"\n",
    "\n",
    "#2. Transfer learning.\n",
    "\n",
    "#a. Create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the \n",
    "softmax output layer with a new one.\n",
    "\n",
    "\"\"\"To create a new Deep Neural Network (DNN) that reuses the pretrained hidden layers from a previous model, freezes them, \n",
    "   and replaces the softmax output layer with a new one, you can follow these steps:\n",
    "   \n",
    "   import tensorflow as tf\n",
    "\n",
    "# Load the pretrained model\n",
    "pretrained_model = tf.keras.models.load_model('mnist_dnn_final_model.h5')\n",
    "\n",
    "# Freeze the pretrained hidden layers\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new DNN by reusing the pretrained hidden layers\n",
    "new_model = tf.keras.models.Sequential()\n",
    "for layer in pretrained_model.layers[:-1]:  # Exclude the output layer\n",
    "    new_model.add(layer)\n",
    "\n",
    "# Add a new softmax output layer for the new task\n",
    "new_model.add(tf.keras.layers.Dense(10, activation='softmax'))  # Assuming 10 classes for the new task\n",
    "\n",
    "# Compile the new model\n",
    "new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  \n",
    "  \n",
    "  In this example, we first load the pretrained model using tf.keras.models.load_model() with the filename of the \n",
    "  saved model file (mnist_dnn_final_model.h5 in this case).\n",
    "\n",
    "  Next, we iterate over each layer of the pretrained model and set their trainable attribute to False. This freezes\n",
    "  the weights of the pretrained hidden layers, preventing them from being updated during training.\n",
    "\n",
    "  Then, we create a new model called new_model and add all the layers from the pretrained model except the last layer\n",
    "  (output layer) using a loop. This effectively reuses the pretrained hidden layers.\n",
    "\n",
    "  Finally, we add a new softmax output layer with the appropriate number of units (assuming 10 classes for the new task)\n",
    "  and compile the new model with the desired optimizer, loss function, and metrics.\"\"\"\n",
    "\n",
    "#b. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small\n",
    "number of examples, can you achieve high precision?\n",
    "\n",
    "\"\"\"Training a new Deep Neural Network (DNN) using transfer learning on a small number of examples can be challenging, \n",
    "   but it's still possible to achieve high precision. However, it's important to note that the performance may vary \n",
    "   depending on the complexity of the task and the amount of available data. Here's an example of how you can train \n",
    "   the new DNN on digits 5 to 9 using only 100 images per digit:\n",
    "   \n",
    "   import time\n",
    "import numpy as np\n",
    "\n",
    "# Load the data for digits 5 to 9\n",
    "X_train_transfer = ...\n",
    "y_train_transfer = ...\n",
    "X_test_transfer = ...\n",
    "y_test_transfer = ...\n",
    "\n",
    "# Select 100 images per digit for training\n",
    "num_examples_per_digit = 100\n",
    "X_train_transfer_subset = []\n",
    "y_train_transfer_subset = []\n",
    "\n",
    "for digit in range(5, 10):\n",
    "    digit_indices = np.where(y_train_transfer == digit)[0]\n",
    "    np.random.shuffle(digit_indices)\n",
    "    selected_indices = digit_indices[:num_examples_per_digit]\n",
    "    X_train_transfer_subset.append(X_train_transfer[selected_indices])\n",
    "    y_train_transfer_subset.append(y_train_transfer[selected_indices])\n",
    "\n",
    "X_train_transfer_subset = np.concatenate(X_train_transfer_subset)\n",
    "y_train_transfer_subset = np.concatenate(y_train_transfer_subset)\n",
    "\n",
    "# Train the new DNN on the subset of data\n",
    "start_time = time.time()\n",
    "new_model.fit(X_train_transfer_subset, y_train_transfer_subset, epochs=50, batch_size=32, validation_data=(X_test_transfer,\n",
    "y_test_transfer))\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Evaluate the precision of the model\n",
    "precision = new_model.evaluate(X_test_transfer, y_test_transfer)[1]\n",
    "\n",
    "print(\"Training time: {:.2f} seconds\".format(training_time))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "\n",
    "  In this example, you need to replace X_train_transfer, y_train_transfer, X_test_transfer, and y_test_transfer with the \n",
    "  corresponding data for digits 5 to 9. Ensure that the data is appropriately preprocessed and split into training and \n",
    "  testing sets.\n",
    "\n",
    "  The code randomly selects 100 images per digit from the training set and creates a subset for training. This ensures\n",
    "  that you have a balanced subset of data with 100 examples per digit.\n",
    "\n",
    "  The new_model is then trained on this subset using the fit function. Adjust the number of epochs and batch size as needed.\n",
    "\n",
    "  After training, the model is evaluated on the testing set, and the precision is calculated and printed.\n",
    "\n",
    "  Please note that achieving high precision with only 100 examples per digit may be challenging, as the model might not \n",
    "  have sufficient data to generalize well. Increasing the number of training examples or employing techniques like data \n",
    "  augmentation can help improve the performance.\"\"\"\n",
    "\n",
    "#c. Try caching the frozen layers, and train the model again: how much faster is it now?\n",
    "\n",
    "\"\"\"Caching the frozen layers can significantly speed up the training process in transfer learning, as the frozen layers' \n",
    "   outputs can be precomputed and reused for multiple epochs. Here's an example of how you can cache the frozen layers \n",
    "   and train the model again:\n",
    "   \n",
    "   \n",
    "   import time\n",
    "import numpy as np\n",
    "\n",
    "# Load the data for digits 5 to 9\n",
    "X_train_transfer = ...\n",
    "y_train_transfer = ...\n",
    "X_test_transfer = ...\n",
    "y_test_transfer = ...\n",
    "\n",
    "# Select 100 images per digit for training\n",
    "num_examples_per_digit = 100\n",
    "X_train_transfer_subset = []\n",
    "y_train_transfer_subset = []\n",
    "\n",
    "for digit in range(5, 10):\n",
    "    digit_indices = np.where(y_train_transfer == digit)[0]\n",
    "    np.random.shuffle(digit_indices)\n",
    "    selected_indices = digit_indices[:num_examples_per_digit]\n",
    "    X_train_transfer_subset.append(X_train_transfer[selected_indices])\n",
    "    y_train_transfer_subset.append(y_train_transfer[selected_indices])\n",
    "\n",
    "X_train_transfer_subset = np.concatenate(X_train_transfer_subset)\n",
    "y_train_transfer_subset = np.concatenate(y_train_transfer_subset)\n",
    "\n",
    "# Cache the outputs of the frozen layers\n",
    "X_train_frozen = pretrained_model.predict(X_train_transfer_subset)\n",
    "\n",
    "# Train the new model using cached frozen layer outputs\n",
    "start_time = time.time()\n",
    "new_model.fit(X_train_frozen, y_train_transfer_subset, epochs=50, batch_size=32, validation_data=(X_test_transfer, y_test_\n",
    "transfer))\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Evaluate the precision of the model\n",
    "precision = new_model.evaluate(X_test_transfer, y_test_transfer)[1]\n",
    "\n",
    "print(\"Training time (with caching): {:.2f} seconds\".format(training_time))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "\n",
    "  In this example, after selecting the subset of data for training, we precompute the outputs of the frozen layers for \n",
    "  that subset using the pretrained model's predict method. These outputs (X_train_frozen) are then used as input to train\n",
    "  the new model.\n",
    "\n",
    "  By caching the frozen layer outputs, we avoid recomputing them in each epoch, leading to faster training.\n",
    "\n",
    "  The rest of the code remains the same, evaluating the precision of the model on the testing set and printing the training\n",
    "  time and precision.\n",
    "\n",
    "  Please note that the first run of training with caching might be slower than subsequent runs due to additional computations\n",
    "  involved in caching. The subsequent runs, however, should demonstrate significant speedup compared to training without\n",
    "  caching.\n",
    "\n",
    "  Ensure that you have the necessary data and adapt the code accordingly to run the training and evaluation successfully.\"\"\"\n",
    "\n",
    "#d. Try again reusing just four hidden layers instead of five. Can you achieve a higher precision?\n",
    "\n",
    "\"\"\"When reusing a smaller number of hidden layers instead of all the layers in transfer learning, the performance can\n",
    "   vary depending on the specific task and dataset. It's possible to achieve a higher precision in some cases, as \n",
    "   reducing the complexity of the model may help prevent overfitting or improve generalization. Here's an example of \n",
    "   reusing only four hidden layers:\n",
    "   \n",
    "   import tensorflow as tf\n",
    "\n",
    "# Load the pretrained model\n",
    "pretrained_model = tf.keras.models.load_model('mnist_dnn_final_model.h5')\n",
    "\n",
    "# Freeze the pretrained hidden layers\n",
    "for layer in pretrained_model.layers[:-4]:  # Freeze all but the last four layers\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new DNN by reusing four pretrained hidden layers\n",
    "new_model = tf.keras.models.Sequential(pretrained_model.layers[:-4])\n",
    "\n",
    "# Add a new softmax output layer for the new task\n",
    "new_model.add(tf.keras.layers.Dense(10, activation='softmax'))  # Assuming 10 classes for the new task\n",
    "\n",
    "# Compile the new model\n",
    "new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  \n",
    "  \n",
    "  In this example, we modify the code from the previous step by freezing all but the last four layers of the \n",
    "  pretrained model. We achieve this by selecting pretrained_model.layers[:-4] when creating the new model.\n",
    "\n",
    "  The remaining steps, including adding the new softmax output layer and compiling the model, remain the same.\n",
    "\n",
    "  By reusing a smaller number of hidden layers, you introduce more flexibility to the model's architecture, which\n",
    "  can potentially improve its performance. However, it's important to note that the optimal configuration depends\n",
    "  on the specific task and dataset, and experimentation is often required to find the best combination of layers \n",
    "  for your particular use case.\n",
    "\n",
    "  After making these modifications, you can proceed to train and evaluate the new model using the training and testing \n",
    "  data specific to your task.\"\"\"\n",
    "\n",
    "#e. Now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?\n",
    "\n",
    "\"\"\"Unfreezing the top two hidden layers and continuing the training can allow the model to adapt and fine-tune these \n",
    "   layers to the new task, potentially leading to improved performance. Here's an example of how you can unfreeze the \n",
    "   top two hidden layers and continue training:\n",
    "   \n",
    "   # Unfreeze the top two hidden layers\n",
    "for layer in new_model.layers[-2:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile the model after unfreezing the layers\n",
    "new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue training the model\n",
    "history = new_model.fit(X_train_transfer, y_train_transfer, epochs=50, batch_size=32, validation_data=(X_test_transfer,\n",
    "y_test_transfer))\n",
    "\n",
    "\n",
    "    In this example, we unfreeze the top two hidden layers of the new_model by setting their trainable attribute to True. \n",
    "    This allows these layers to be updated during training.\n",
    "\n",
    "    After unfreezing the layers, we compile the model again with the desired optimizer, loss function, and metrics.\n",
    "\n",
    "   Finally, we continue training the model using the training and testing data (X_train_transfer, y_train_transfer, X_test_\n",
    "   transfer, y_test_transfer) for additional epochs.\n",
    "\n",
    "   By fine-tuning the top layers, the model can learn task-specific patterns and potentially improve its performance on \n",
    "   the new task.\n",
    "\n",
    "   Feel free to adjust the number of epochs, batch size, and other training parameters according to your specific requirements.\n",
    "\n",
    "  Please note that fine-tuning the top layers may require careful consideration, as too much modification to the pretrained\n",
    "  layers can lead to overfitting or loss of previously learned representations. It's recommended to monitor the validation\n",
    "  performance and consider early stopping or regularization techniques if needed.\"\"\"\n",
    "\n",
    "#3. Pretraining on an auxiliary task.\n",
    "\n",
    "#a. In this exercise you will build a DNN that compares two MNIST digit images and predicts whether they represent the \n",
    "same digit or not. Then you will reuse the lower layers of this network to train an MNIST classifier using very little \n",
    "training data. Start by building two DNNs (let’s call them DNN A and B), both similar to the one you built earlier but \n",
    "without the output layer: each DNN should have five hidden layers of 100 neurons each, He initialization, and ELU\n",
    "activation. Next, add one more hidden layer with 10 units on top of both DNNs. To do this, you should use\n",
    "TensorFlow’s concat() function with axis=1 to concatenate the outputs of both DNNs for each instance, then feed the\n",
    "result to the hidden layer. Finally, add an output layer with a single neuron using the logistic activation function.\n",
    "\n",
    "\"\"\"To build DNN A and DNN B for comparing MNIST digit images and predicting whether they represent the same digit or not,\n",
    "   you can follow these steps:\n",
    "   \n",
    "   \n",
    "   import tensorflow as tf\n",
    "\n",
    "# Create DNN A\n",
    "dnn_a = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "])\n",
    "\n",
    "# Create DNN B\n",
    "dnn_b = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "])\n",
    "\n",
    "# Concatenate outputs of DNN A and DNN B\n",
    "concatenated = tf.keras.layers.Concatenate(axis=1)([dnn_a.output, dnn_b.output])\n",
    "\n",
    "# Add an additional hidden layer\n",
    "hidden_layer = tf.keras.layers.Dense(10, activation='elu', kernel_initializer='he_normal')(concatenated)\n",
    "\n",
    "# Output layer\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Create the final model\n",
    "model = tf.keras.models.Model(inputs=[dnn_a.input, dnn_b.input], outputs=output)\n",
    "\n",
    "  \n",
    "  DNN consists of five hidden layers with 100 neurons, ELU activation, and He initialization.\n",
    "\n",
    "  To combine the outputs of DNN A and DNN B, we use the Concatenate layer from TensorFlow, specifying axis=1 to \n",
    "  concatenate along the feature axis.\n",
    "\n",
    "  Next, an additional hidden layer with 10 units is added on top of the concatenated outputs using the Dense layer.\n",
    "\n",
    "  Finally, an output layer with a single neuron and a sigmoid activation function is added to predict whether the\n",
    "  two digit images represent the same digit or not.\n",
    "\n",
    "  This configuration allows us to compare two digit images by passing them as inputs to DNN A and DNN B and then \n",
    "  combining their outputs to make the final prediction.\"\"\"\n",
    "\n",
    "#b. Split the MNIST training set in two sets: split #1 should containing 55,000 images, and split #2 should contain \n",
    "contain 5,000 images. Create a function that generates a training batch where each instance is a pair of MNIST images \n",
    "picked from split #1. Half of the training instances should be pairs of images that belong to the same class, while \n",
    "the other half should be images from different classes. For each pair, the training label should be 0 if the images \n",
    "are from the same class, or 1 if they are from different classes.\n",
    "\n",
    "\"\"\"To generate a training batch with pairs of MNIST images from split #1, where half of the instances have images \n",
    "   from the same class and the other half have images from different classes, you can use the following function:\n",
    "   \n",
    "   import numpy as np\n",
    "\n",
    "def generate_training_batch(X_train, y_train, batch_size=32):\n",
    "    num_samples = batch_size // 2\n",
    "    num_classes = 10\n",
    "    \n",
    "    # Select random indices for same-class pairs\n",
    "    same_class_indices = np.random.choice(len(X_train), size=num_samples, replace=False)\n",
    "    \n",
    "    # Generate same-class pairs\n",
    "    same_class_pairs = []\n",
    "    same_class_labels = []\n",
    "    \n",
    "    for idx in same_class_indices:\n",
    "        class_label = y_train[idx]\n",
    "        class_indices = np.where(y_train == class_label)[0]\n",
    "        other_idx = np.random.choice(class_indices)\n",
    "        \n",
    "        same_class_pairs.append([X_train[idx], X_train[other_idx]])\n",
    "        same_class_labels.append(0)  # Label 0 for same-class pairs\n",
    "    \n",
    "    # Generate different-class pairs\n",
    "    different_class_pairs = []\n",
    "    different_class_labels = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        class_labels = np.random.choice(num_classes, size=2, replace=False)\n",
    "        class_indices_1 = np.where(y_train == class_labels[0])[0]\n",
    "        class_indices_2 = np.where(y_train == class_labels[1])[0]\n",
    "        \n",
    "        idx_1 = np.random.choice(class_indices_1)\n",
    "        idx_2 = np.random.choice(class_indices_2)\n",
    "        \n",
    "        different_class_pairs.append([X_train[idx_1], X_train[idx_2]])\n",
    "        different_class_labels.append(1)  # Label 1 for different-class pairs\n",
    "    \n",
    "    # Combine same-class and different-class pairs\n",
    "    pairs = same_class_pairs + different_class_pairs\n",
    "    labels = same_class_labels + different_class_labels\n",
    "    \n",
    "    # Shuffle the pairs and labels\n",
    "    indices = np.random.permutation(len(pairs))\n",
    "    pairs = np.array(pairs)[indices]\n",
    "    labels = np.array(labels)[indices]\n",
    "    \n",
    "    # Split the pairs into two separate arrays\n",
    "    pairs_1 = pairs[:, 0]\n",
    "    pairs_2 = pairs[:, 1]\n",
    "    \n",
    "    return pairs_1, pairs_2, labels\n",
    "\n",
    "   This function takes the training data X_train and corresponding labels y_train from split #1 as input and generates \n",
    "   a batch of pairs of MNIST images along with their labels. The batch_size determines the number of pairs to generate.\n",
    "\n",
    "   Half of the instances in the batch are pairs of images that belong to the same class (same_class_pairs), and the \n",
    "   other half are pairs of images from different classes (different_class_pairs). The labels for same-class pairs are \n",
    "   set to 0, and the labels for different-class pairs are set to 1.\n",
    "\n",
    "   The function returns two arrays (pairs_1 and pairs_2) representing the pairs of images, and a labels array.\"\"\"\n",
    "\n",
    "#c. Train the DNN on this training set. For each image pair, you can simultaneously feed the first image to DNN A and \n",
    "the second image to DNN B. The whole network will gradually learn to tell whether two images belong to the same class or not.\n",
    "\n",
    "\"\"\"Pretraining on an auxiliary task is a technique commonly used in deep learning to enhance the performance of a neural\n",
    "   network on the main task. In the context you mentioned, the goal is to train a Deep Neural Network (DNN) to determine\n",
    "   whether two images belong to the same class or not.\n",
    "\n",
    "   The process typically involves the following steps:\n",
    "\n",
    "   1. Data collection: Gather a large dataset of image pairs, where each pair consists of two images. For the purpose of \n",
    "      training the network, these pairs should be labeled to indicate whether the images belong to the same class or not.\n",
    "\n",
    "   2. Architecture setup: Set up two separate DNN models, referred to as DNN A and DNN B. These models will process the\n",
    "      first and second images from each pair, respectively.\n",
    "\n",
    "   3. Pretraining: Pretrain DNN A and DNN B on a different auxiliary task, which is typically easier or related to the \n",
    "      main task. This pretraining step helps the models to learn useful representations that can be applied to the main task.\n",
    "\n",
    "   4. Training: Once the pretraining is complete, the network can be trained on the task of determining whether two images\n",
    "      belong to the same class. For each image pair in the training set, you simultaneously feed the first image to DNN A \n",
    "      and the second image to DNN B.\n",
    "\n",
    "   5. Loss function: Define a suitable loss function to quantify the dissimilarity between the output of DNN A and DNN B.\n",
    "      This loss function will guide the training process and enable the network to learn to discriminate between same-class \n",
    "      and different-class image pairs.\n",
    "\n",
    "   6. Backpropagation and optimization: Compute the gradients of the loss function with respect to the parameters of both \n",
    "      DNN A and DNN B using backpropagation. Update the model parameters using an optimization algorithm, such as stochastic \n",
    "      gradient descent (SGD) or Adam, to minimize the loss.\n",
    "\n",
    "   7. Iterative training: Repeat the training process for multiple iterations or epochs, gradually adjusting the model \n",
    "      parameters to improve performance.\n",
    "\n",
    "  By training the network using this approach, the whole network learns to extract meaningful features from the images and \n",
    "  to make accurate predictions regarding their class similarity. The pretraining step helps initialize the models with useful\n",
    "  knowledge, leading to improved performance on the main task.\"\"\"\n",
    "\n",
    "#d. Now create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer on top with \n",
    "10 neurons. Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class.\n",
    "\n",
    "\"\"\"Pretraining on an auxiliary task followed by fine-tuning is a common approach to achieve high performance on a main \n",
    "   task, even with limited data. In this case, you can follow the steps below to create a new DNN by reusing and freezing \n",
    "   the hidden layers of DNN A, and then train it on split #2 with only 500 images per class:\n",
    "\n",
    "   1. Pretraining: Train DNN A on a related task using a large dataset. This pretraining step helps DNN A learn useful\n",
    "      representations that can be transferred to the main task.\n",
    "\n",
    "   2. Freeze hidden layers: After pretraining, freeze the parameters of the hidden layers in DNN A. This prevents their \n",
    "      weights from being updated during the subsequent training process.\n",
    "\n",
    "   3. Softmax output layer: Add a new softmax output layer on top of the frozen hidden layers of DNN A. This new layer \n",
    "      will have 10 neurons, corresponding to the number of classes in your classification problem.\n",
    "\n",
    "   4. Split #2 data: Divide your dataset into different splits for training and evaluation. In this case, use split #2 \n",
    "      for training, which contains only 500 images per class.\n",
    "\n",
    "   5. Training: Train the new DNN, referred to as DNN B, using split #2. Since the hidden layers are frozen, only the\n",
    "      weights of the new softmax output layer will be updated during training.\n",
    "\n",
    "   6. Loss function and optimization: Define a suitable loss function for multi-class classification, such as categorical\n",
    "      cross-entropy, and an optimization algorithm like SGD or Adam. Use these to update the weights of the output layer\n",
    "      during training.\n",
    "\n",
    "   7. Iterative training: Repeat the training process for multiple iterations or epochs, adjusting the weights of the \n",
    "      output layer to minimize the loss on the training data.\n",
    "\n",
    "  By reusing and freezing the hidden layers of DNN A, the new DNN B starts with prelearned features that can be highly \n",
    "  beneficial, especially when the available data is limited. Training DNN B on split #2 allows it to adapt to the specific \n",
    "  task at hand, even though the amount of data per class is only 500 images.\n",
    "\n",
    "  Despite having a smaller dataset, fine-tuning the pretrained model on split #2 can potentially lead to high performance,\n",
    "  as the network can leverage the knowledge and representations learned from the auxiliary task. However, the exact \n",
    "  performance achieved will depend on the complexity of the main task, the quality of the pretrained model, and the \n",
    "  similarity between the auxiliary and main tasks.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
